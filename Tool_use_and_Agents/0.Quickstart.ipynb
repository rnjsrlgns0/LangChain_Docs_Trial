{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi agent 지식알림 챗봇 만들기를 위한 연습입니다^^\n",
    "- LangChain 공식 docs 중 Use cases/Tool use and agents의 문서 사용\n",
    "- 중간중간 공식 docs와 다른 시도가 끼어 있을 수 있습니다......?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../dot.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool 만들기\n",
    "- 정해져 있는 tool 말고도 필요에 따른 사용자 정의 tool 구성 가능\n",
    "- @tool 아래 라인에 함수 형태로 정의\n",
    "- 아래 셀 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers together.\"\"\" #-> tool에 대한 description\n",
    "    return a * b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool의 메타데이터 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply\n",
      "Multiply two numbers together.\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "print(multiply.name)  # tool의 이름을 출력합니다.\n",
    "print(multiply.description)  # tool의 설명을 출력합니다.\n",
    "print(multiply.args)  # tool의 인자 정보를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool에 인자를 넣을 때 잘 들어가지 않는다면???\n",
    "- print('tool_name'.args) 후 인자 살펴보기 \n",
    "- 반드시 위 함수 실행 결과에 따른 형식에 따라 입력해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply.invoke({\"a\" : 2, \"b\" : 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 간단한 Chain 만들어.보기 \n",
    "![Agent_Tool_Chains](imgs/Agent_tool_Chain.png)\n",
    "- 예제에서는 chatgpt를 사용, but 본인은 xionic에서 공개한 llama-3-70B를 쓰기로 했습니다.\n",
    "- 여기저기서 쓰다보면 토큰이 무지하게 나오니까....\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 정의\n",
    "from langchain_openai import ChatOpenAI\n",
    "llama3 = ChatOpenAI(\n",
    "    base_url = 'http://sionic.chat:8001/v1',\n",
    "    api_key = \"934c4bbc-c384-4bea-af82-1450d7f8128d\",\n",
    "    model = 'xionic-ko-llama-3-70b',\n",
    "    temperature = 0.1,\n",
    ")\n",
    "# dolphine = ChatOpenAI(\n",
    "#     base_url = 'http://localhost:1234/v1',\n",
    "#     api_key = \"lm-studio\",\n",
    "#     model = 'cognitivecomputations/dolphin-2.9-llama3-8b-gguf',\n",
    "#     temperature = 0,\n",
    "# )\n",
    "\n",
    "llm = ChatOpenAI(model = 'gpt-3.5-turbo')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM과 정의된 TOOL을 묶는법\n",
    "# LLM.bind_tools(['tool_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='2 × 3 = 6' response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 20, 'total_tokens': 28}, 'model_name': 'xionic-ko-llama-3-70b', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-26ac7d06-9ed9-459c-a9a3-4f60e7580ad0-0' usage_metadata={'input_tokens': 20, 'output_tokens': 8, 'total_tokens': 28} /n\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_1aUx0VHQtrjqrEETRK4nKP7H', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 56, 'total_tokens': 73}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None} id='run-51f23efe-639f-45a9-bd9d-b38527401063-0' tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_1aUx0VHQtrjqrEETRK4nKP7H'}] usage_metadata={'input_tokens': 56, 'output_tokens': 17, 'total_tokens': 73}\n"
     ]
    }
   ],
   "source": [
    "# LLM과 정의된 TOOL을 bind\n",
    "llama3_with_tools = llama3.bind_tools([multiply])\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "# dolphine_with_tools = dolphine.bind_tools([multiply])\n",
    "\n",
    "# message를 넣어 LLM이 tool에 제대로 query를 넣는지 확인\n",
    "msg = llm_with_tools.invoke(\"2와 3을 곱해줘\")\n",
    "# msg_dolphine = dolphine_with_tools.invoke(\"2와 3을 곱해줘\")\n",
    "msg_llama_3 = llama3_with_tools.invoke(\"2와 3을 곱해줘\")\n",
    "print(msg_llama_3, '/n')\n",
    "print(msg)\n",
    "# print('\\n', msg_dolphine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': None,\n",
       " 'bound': ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x3254c3c50>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x3254ccdd0>, openai_api_key=SecretStr('**********'), openai_proxy=''),\n",
       " 'kwargs': {'tools': [{'type': 'function',\n",
       "    'function': {'name': 'multiply',\n",
       "     'description': 'multiply(a: int, b: int) -> int - Multiply two numbers together.',\n",
       "     'parameters': {'type': 'object',\n",
       "      'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}},\n",
       "      'required': ['a', 'b']}}}]},\n",
       " 'config': {},\n",
       " 'config_factories': [],\n",
       " 'custom_input_type': None,\n",
       " 'custom_output_type': None}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 2, 'b': 3},\n",
       "  'id': 'call_bdk6KpSy9uCpDtzyYcUsXU0V'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama-3-xionic의 경우, bind_tools method 적용 시, tool을 제대로 인식하지 못하는 것 확인\n",
    "## bind_tools method 사용 부분은 chatgpt-3.5-turbo 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 2, 'b': 3},\n",
       "  'id': 'call_bdk6KpSy9uCpDtzyYcUsXU0V'}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoking the tool\n",
    "- 훌륭해요! 우리는 도구 호출을 생성할 수 있습니다. 그러나 실제로 도구를 호출하고 싶다면 어떻게 해야 할까요? 이를 위해 생성된 도구 인자를 우리의 도구에 전달해야 합니다. 간단한 예로, 첫 번째 tool_call의 인자를 추출해 보겠습니다:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# llm_with_tools에서 tool_calls의 첫 번째 호출을 가져와서 그 인자들을 multiply 함수에 전달합니다.\n",
    "# -> 그렇다네요 chain의 출력값 연쇄는 항상 헷갈립니다 허허....\n",
    "chain = llm_with_tools | (lambda x: x.tool_calls[0][\"args\"]) | multiply\n",
    "chain.invoke('23*4는뭐야?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents\n",
    "![Agent_Tool_Agents](imgs/Agent_tool_Agents.png)\n",
    "- 특정 사용자 입력에 필요한 도구 사용 순서를 알고 있을 때 체인은 매우 유용합니다. 그러나 특정 사용 사례에서는 입력에 따라 도구를 사용하는 횟수가 달라집니다. 이러한 경우에는 모델이 도구를 몇 번 사용할지, 어떤 순서로 사용할지 스스로 결정하게 하고 싶습니다. 에이전트는 바로 이를 가능하게 합니다.\n",
    "- LangChain은 다양한 사용 사례에 최적화된 여러 내장 에이전트를 제공합니다. 여기에서 모든 에이전트 유형에 대해 읽어보세요. \n",
    "- 우리는 도구 호출 에이전트를 사용할 것입니다. 이는 일반적으로 가장 신뢰할 수 있는 종류로, 대부분의 사용 사례에 권장됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Assistant is a large language model trained by OpenAI.\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input', 'tool_names', 'tools'], template='TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n{tools}\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": string, \\\\ The action to take. Must be one of {tool_names}\\n    \"action_input\": string \\\\ The input to the action\\n}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\ You should put what you want to return to use here. You must write the response in Korean.\\n}}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\nUse search tools to answer the following question in Korean:\\n{input}')),\n",
       " SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad'], template='Write in Korean:\\n{agent_scratchpad}'))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.agents import create_json_chat_agent\n",
    "# xionic의 llama-3 모델으로 agent 생성 시 create_json_agent 메소드를 사용해야하며,\n",
    "# AgentExecutor를 사용해야함\n",
    "# json prompt-ko -> 테디노트님 작품\n",
    "json_prompt_ko = hub.pull('teddynote/react-chat-json-korean')\n",
    "# prompt 의 messages를 출력합니다.\n",
    "json_prompt_ko.messages\n",
    "# json_prompt = hub.pull(\"hwchase17/react-chat-json\") \n",
    "# json_prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents는 여러 도구를 사용하기 쉽다는 점에서 장점이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"두 수를 더합니다.\"\"\"\n",
    "    tool_name = 'exponentiate'\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def exponentiate(a: int, b: int) -> int:\n",
    "    \"\"\"두 수를 거듭제곱합니다.\"\"\"\n",
    "    tool_name = 'exponentiate'\n",
    "    return a ** b\n",
    "\n",
    "tools = [add, \n",
    "         exponentiate, \n",
    "         multiply \n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tool_calling_agent는 여러 도구를 사용하여 사용자의 요청을 처리할 수 있는 에이전트입니다.\n",
    "- 이 에이전트는 사용자의 입력을 분석하고, 필요한 도구를 호출하여 적절한 작업을 수행합니다.\n",
    "- 예를 들어, 계산이나 데이터 검색과 같은 작업을 자동으로 처리할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "# llama_3_agent = create_json_chat_agent(llama3, tools, json_prompt_ko) #xionic-ko-llama-3-70b 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama3_agent_executor = AgentExecutor(agent = llama_3_agent, \n",
    "#                                tools = tools, \n",
    "#                                verbose=True,\n",
    "#                                handle_parsing_errors = True,\n",
    "#                                return_intermediate_steps = True\n",
    "#                                )\n",
    "agent_executor = AgentExecutor(agent = agent, \n",
    "                               tools = tools, \n",
    "                               verbose=True,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `exponentiate` with `{'a': 3, 'b': 5}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m243\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `add` with `{'a': 12, 'b': 3}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m15\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `multiply` with `{'a': 243, 'b': 15}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m3645\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `exponentiate` with `{'a': 3645, 'b': 2}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m13286025\u001b[0m\u001b[32;1m\u001b[1;3mThe result of taking 3 to the fifth power and multiplying that by the sum of twelve and three, then squaring the whole result is 13,286,025.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "답변: The result of taking 3 to the fifth power and multiplying that by the sum of twelve and three, then squaring the whole result is 13,286,025.\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Take 3 to the fifth power and multiply that by the sum of twelve and three, then square the whole result\"\n",
    "    }\n",
    ")\n",
    "print(f'답변: {response[\"output\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Teddy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
