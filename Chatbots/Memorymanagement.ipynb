{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory 관리에 관한 실습코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../dot.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat 모델은 llama-3-ko 7b모델을 로컬로 사용해봤습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# chat = ChatOpenAI(model = \"gpt-3.5-turbo-1106\")\n",
    "chat = ChatOpenAI(base_url=\"http://localhost:1234/v1\",\n",
    "                  api_key=\"lm-studio\",\n",
    "                  model = \"cognitivecomputations/dolphin-2.9-llama3-8b-gguf\",\n",
    "                  temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I said, \"J\\'adore la programmation,\" which translates to \"I love programming\" in English.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"you are a helpful assistant. Answer all questions to the best of your ability\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content = \"Translate this sentence from English to French: I love programming\"\n",
    "            ),\n",
    "            AIMessage(content = \"J'adore la programmation.\"),\n",
    "            HumanMessage(content = \"What did you just say?\"),\n",
    "        ],\n",
    "    }\n",
    ").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat history\n",
    "- 메시지를 직접 배열로 저장하고 전달하는 것도 괜찮지만, LangChain의 내장 메시지 기록 클래스를 사용하여 메시지를 저장하고 로드할 수도 있습니다. \n",
    "- 이 클래스의 인스턴스는 영속적 저장소에서 채팅 메시지를 저장하고 로드하는 역할을 합니다. \n",
    "- LangChain은 다양한 제공업체와 통합되어 있습니다 \n",
    "- 여기에서 통합 목록을 볼 수 있습니다 \n",
    "- 하지만 이 데모에서는 일회성 데모 클래스를 사용할 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Tlanslate this sentence from English to French: I love programming'),\n",
       " AIMessage(content=\"J'adore la programmation.\")]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "chat_history = ChatMessageHistory()\n",
    "chat_history.add_user_message(\n",
    "    \"Tlanslate this sentence from English to French: I love programming\"\n",
    ")\n",
    "chat_history.add_ai_message(\"J'adore la programmation.\")\n",
    "chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked me to translate an English sentence into French, which was \"I love programming.\" The translation is \"Je aime programmer.\"\\n\\nThen, you asked what did I just ask you, and now you\\'re asking for a response.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1 = \"Translate this sentence from English to French: I love programming.\"\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "chat_history.add_user_message(input_1)\n",
    "response = chain.invoke(\n",
    "    {\n",
    "    \"messages\": chat_history.messages,\n",
    "    }\n",
    ")\n",
    "chat_history.add_ai_message(response) # input_1에 대한 response를 chat_history에 추가\n",
    "\n",
    "input_2 = \"What did I just ask you?\"\n",
    "\n",
    "chat_history.add_user_message(input_2)\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"messages\":chat_history.messages,\n",
    "    }\n",
    ").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # 자동 히스토리 관리\n",
    "- 이전 예제들은 명시적으로 체인에 메시지를 전달합니다.\n",
    "- 이것은 완전히 수용 가능한 방법이지만, 새로운 메시지를 외부에서 관리해야 합니다.\n",
    "- LangChain은 이 프로세스를 자동으로 처리할 수 있는 LCEL 체인을 위한 RunnableWithMessageHistory라는 래퍼도 포함하고 있습니다.\n",
    "- \n",
    "- 이 작동 방식을 보여주기 위해, 위의 프롬프트를 약간 수정하여 채팅 히스토리 이후에 HumanMessage 템플릿을 채우는 최종 입력 변수를 사용하도록 합시다.\n",
    "- 이는 현재 메시지 이전의 모든 메시지를 포함하는 chat_history 매개변수를 예상할 것을 의미합니다:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Teddy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
