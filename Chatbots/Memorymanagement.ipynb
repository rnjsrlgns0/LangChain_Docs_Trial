{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory 관리에 관한 실습코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../dot.env')\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "def _set_if_undefined(var: str):\n",
    "    # 주어진 환경 변수가 설정되어 있지 않다면 사용자에게 입력을 요청하여 설정합니다.\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
    "\n",
    "\n",
    "# OPENAI_API_KEY 환경 변수가 설정되어 있지 않으면 사용자에게 입력을 요청합니다.\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")\n",
    "# LANGCHAIN_API_KEY 환경 변수가 설정되어 있지 않으면 사용자에게 입력을 요청합니다.\n",
    "_set_if_undefined(\"LANGCHAIN_API_KEY\")\n",
    "# TAVILY_API_KEY 환경 변수가 설정되어 있지 않으면 사용자에게 입력을 요청합니다.\n",
    "_set_if_undefined(\"TAVILY_API_KEY\")\n",
    "\n",
    "# LangSmith 추적 기능을 활성화합니다. (선택적)\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Chatbots_Memory_Management\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat 모델은 llama-3-ko 8b모델을 로컬로 사용해봤습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# chat = ChatOpenAI(model = \"gpt-3.5-turbo-1106\")\n",
    "# chat = ChatOpenAI(base_url=\"http://localhost:1234/v1\",\n",
    "#                   api_key=\"lm-studio\",\n",
    "#                   model = \"beomi_llama_3_ko/Llama-3-Open-Ko-8B-Q8_0\",\n",
    "#                   temperature=0.0)\n",
    "chat = ChatOpenAI(base_url=\"http://localhost:1234/v1\",\n",
    "                  api_key=\"lm-studio\",\n",
    "                  model =  \"asiansoul_q8_0/Joah-Remix-Llama-3-KoEn-8B-Reborn-8B-Q8_0\",\n",
    "                  temperature=0.0)\n",
    "# chat = ChatOpenAI(base_url=\"http://localhost:1234/v1\",\n",
    "#                   api_key=\"lm-studio\",\n",
    "#                   model=\"teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf\",\n",
    "#                   temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I translated the sentence \"I love programming\" into French, and it came out as \"J\\'adore la programmation\". This means the same thing in both languages - that I enjoy or have a strong affection for programming.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"you are a helpful assistant. Answer all questions to the best of your ability\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content = \"Translate this sentence from English to French: I love programming\"\n",
    "            ),\n",
    "            AIMessage(content = \"J'adore la programmation.\"),\n",
    "            HumanMessage(content = \"What did you just say?\"),\n",
    "        ],\n",
    "    }\n",
    ").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat history\n",
    "- 메시지를 직접 배열로 저장하고 전달하는 것도 괜찮지만, LangChain의 내장 메시지 기록 클래스를 사용하여 메시지를 저장하고 로드할 수도 있습니다. \n",
    "- 이 클래스의 인스턴스는 영속적 저장소에서 채팅 메시지를 저장하고 로드하는 역할을 합니다. \n",
    "- LangChain은 다양한 제공업체와 통합되어 있습니다 \n",
    "- 여기에서 통합 목록을 볼 수 있습니다 \n",
    "- 하지만 이 데모에서는 일회성 데모 클래스를 사용할 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Tlanslate this sentence from English to French: I love programming'),\n",
       " AIMessage(content=\"J'adore la programmation.\")]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "chat_history = ChatMessageHistory()\n",
    "chat_history.add_user_message(\n",
    "    \"Tlanslate this sentence from English to French: I love programming\"\n",
    ")\n",
    "chat_history.add_ai_message(\"J'adore la programmation.\")\n",
    "chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked me to translate the sentence \"I love programming\" from English to French.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1 = \"Translate this sentence from English to French: I love programming.\"\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "chat_history.add_user_message(input_1)\n",
    "response = chain.invoke(\n",
    "    {\n",
    "    \"messages\": chat_history.messages,\n",
    "    }\n",
    ")\n",
    "chat_history.add_ai_message(response) # input_1에 대한 response를 chat_history에 추가\n",
    "\n",
    "input_2 = \"What did I just ask you?\"\n",
    "\n",
    "chat_history.add_user_message(input_2)\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"messages\":chat_history.messages,\n",
    "    }\n",
    ").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자동 히스토리 관리\n",
    "- 이전 예제들은 명시적으로 체인에 메시지를 전달합니다.\n",
    "- 이것은 완전히 수용 가능한 방법이지만, 새로운 메시지를 외부에서 관리해야 합니다.\n",
    "- LangChain은 이 프로세스를 자동으로 처리할 수 있는 LCEL 체인을 위한 RunnableWithMessageHistory라는 래퍼도 포함하고 있습니다.\n",
    "- 이 작동 방식을 보여주기 위해, 위의 프롬프트를 약간 수정하여 채팅 히스토리 이후에 HumanMessage 템플릿을 채우는 최종 입력 변수를 사용하도록 합시다.\n",
    "- 이는 현재 메시지 이전의 모든 메시지를 포함하는 chat_history 매개변수를 예상할 것을 의미합니다:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat history를 직접관리하기 위한 prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\" ,\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), # -> message place holder를 인자로 추가 + chat history넣기\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "chat_with_history = ChatMessageHistory()\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: chat_with_history,\n",
    "    input_messages_key= 'input',\n",
    "    history_messages_key= 'chat_history'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이 클래스는 래핑하려는 체인 외에 몇 가지 매개변수를 사용합니다: \n",
    "- 세션 ID에 대한 메시지 히스토리를 반환하는 팩토리 함수입니다. 이를 통해 체인이 다양한 대화에 대해 서로 다른 메시지를 로드하여 한 번에 여러 사용자를 처리할 수 있습니다.\n",
    "- 추적하고 채팅 히스토리에 저장해야 하는 입력의 어느 부분을 지정하는 input_messages_key입니다. 이 예에서는 입력으로 전달된 문자열을 추적하려고 합니다.\n",
    "- 이전 메시지가 프롬프트로 주입되어야 하는 history_messages_key입니다. 우리의 프롬프트에는 chat_history라는 MessagesPlaceholder가 있으므로 이 속성을 일치시킵니다.\n",
    "- (여러 출력을 가진 체인의 경우) 히스토리로 저장할 출력을 지정하는 output_messages_key입니다. 이는 input_messages_key의 역입니다.\n",
    "- 이 새로운 체인을 일반적으로 호출할 수 있으며, 팩토리 함수에 전달할 특정 세션 ID를 지정하는 추가 구성 가능 필드가 있습니다. 이는 데모에서 사용되지 않지만, 실제 체인에서는 전달된 세션에 해당하는 채팅 히스토리를 반환해야 합니다:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Je aime programmer.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke(\n",
    "    {\"input\": \"Translate this sentence from English to French: I love programming.\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I just asked you to translate the sentence \"I love programming\" from English to French.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke(\n",
    "    {\"input\": \"What did I just ask you?\"}, {\"configurable\": {\"session_id\": \"unused\"}}\n",
    ").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chathistory 수정\n",
    " - 저장된 채팅 메시지를 수정하면 챗봇이 다양한 상황을 처리하는 데 도움이 됩니다. 몇 가지 예시는 다음과 같습니다:\n",
    " \n",
    "# 메시지 자르기\n",
    " - LLMs와 채팅 모델은 제한된 컨텍스트 창을 가지고 있으며, 직접적인 제한에 직면하지 않더라도 모델이 처리해야 하는 방해 요소의 양을 제한하고 싶을 수 있습니다.\n",
    " - 하나의 해결책은 가장 최근 n개의 메시지만 로드하고 저장하는 것입니다. 몇 가지 미리로드된 메시지가 있는 예시 히스토리를 사용해 봅시다:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"hey there! I'm Nemo.\"),\n",
       " AIMessage(content='Hello.'),\n",
       " HumanMessage(content='How are you today?'),\n",
       " AIMessage(content='Fine thanks!')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chat history가 너무 길어지면 이후 질문에서 모델이 받아들일 수 있는 토큰값을 초과할 수 있음 -> 잘라주거나 요약해야함\n",
    "\n",
    "# chat history 초기화\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "chat_history.add_user_message(\"hey there! I'm Nemo.\")\n",
    "chat_history.add_ai_message(\"Hello.\")\n",
    "chat_history.add_user_message(\"How are you today?\")\n",
    "chat_history.add_ai_message(\"Fine thanks!\")\n",
    "\n",
    "chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 51134506-7dba-4cd9-8b5c-592aa7401837 not found for run 3727d43e-ba30-46e0-805b-6fd738d01cad. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Nemo.', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 6, 'total_tokens': 12}, 'model_name': 'asiansoul_q8_0/Joah-Remix-Llama-3-KoEn-8B-Reborn-8B-Q8_0', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-46a3e819-ef96-4115-bffe-9986fa816457-0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt  = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),  #context\n",
    "        (\"human\",\"{input}\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | chat\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda sessin_id: chat_history, #message placeholder의 값을 반환하는 함수\n",
    "    input_messages_key = \"input\",\n",
    "    history_messages_key = \"chat_history\"\n",
    ")\n",
    "\n",
    "chain_with_history.invoke(\n",
    "    {\"input\":\"What's my name?\"},\n",
    "    {\"configurable\": {\"session_id\":\"unused\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['chat_history', 'input'] input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant. Answer all questions to the best of your ability.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 위 방법으로 전에 입력했던 사용자의 이름을 불러올 수 있습니다.\n",
    "# 또한 메시지를 잘라 최근 2개의 대화만 context로 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def trim_messages(chain_input):\n",
    "    stored_messages = chat_history.messages\n",
    "    if len(stored_messages) <= 2:\n",
    "        return False\n",
    "    \n",
    "    chat_history.clear()\n",
    "\n",
    "    for message in stored_messages[-2:]:\n",
    "        chat_history.add_message(message)\n",
    "\n",
    "    return True\n",
    "\n",
    "chain_with_trimming = (\n",
    "    RunnablePassthrough.assign(messages_trimmed = trim_messages)\n",
    "    | chain_with_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='P. Sherman lives at 742 Evergreen Terrace.\\n\\n', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 13, 'total_tokens': 24}, 'model_name': 'cognitivecomputations/dolphin-2.9-llama3-8b-gguf', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-dee11fc6-e020-4a5c-b0a4-85a928de2b7e-0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_trimming.invoke(\n",
    "    {\"input\":\"Where does P. Sherman live?\"},\n",
    "    {\"configurable\":{\"session_id\":\"unused\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't know your name, as I am not capable of remembering individual users' information.\", response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 19, 'total_tokens': 38}, 'model_name': 'cognitivecomputations/dolphin-2.9-llama3-8b-gguf', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0ed405f6-5639-41e7-a378-0c78b4eb3652-0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_trimming.invoke(\n",
    "    {\"input\": \"What is my name?\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Where does P. Sherman live?'),\n",
       " AIMessage(content='P. Sherman lives at 742 Evergreen Terrace.\\n\\n', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 13, 'total_tokens': 24}, 'model_name': 'cognitivecomputations/dolphin-2.9-llama3-8b-gguf', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-dee11fc6-e020-4a5c-b0a4-85a928de2b7e-0'),\n",
       " HumanMessage(content='What is my name?'),\n",
       " AIMessage(content=\"I don't know your name, as I am not capable of remembering individual users' information.\", response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 19, 'total_tokens': 38}, 'model_name': 'cognitivecomputations/dolphin-2.9-llama3-8b-gguf', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0ed405f6-5639-41e7-a378-0c78b4eb3652-0')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Memory\n",
    "- 추가 LLM을 통해 제공하는 대화의 요약본을 생성할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"hey there! I'm Nemo.\"),\n",
       " AIMessage(content='Hello.'),\n",
       " HumanMessage(content='How are you today?'),\n",
       " AIMessage(content='Fine thanks!')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chat_history 초기화\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "chat_history.add_user_message(\"hey there! I'm Nemo.\")\n",
    "chat_history.add_ai_message(\"Hello.\")\n",
    "chat_history.add_user_message(\"How are you today?\")\n",
    "chat_history.add_ai_message(\"Fine thanks!\")\n",
    "\n",
    "chat_history.messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt  = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "        (\"user\",\"{input}\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | chat\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda sessin_id: chat_history,\n",
    "    input_messages_key = \"input\",\n",
    "    history_messages_key = \"chat_history\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이제 이전 상호 작용을 요약으로 추출하는 함수를 만들어봅시다. 이것을 체인의 맨 앞에 추가할 수도 있습니다:\n",
    "- 아래 summarize_messages 함수에는 전체 메시지 요약을 위한 새로운 chain이 정의 되어 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables  import RunnablePassthrough\n",
    "def summarize_messages(chain_input):\n",
    "    stored_messages = chat_history.messages\n",
    "    if len(stored_messages) == 0:\n",
    "        return False\n",
    "    \n",
    "    summarization_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                # \"Distill the above chat messages into a single summary message. Include as many specific details as you can\" #<-예제의 prompt\n",
    "                \"Distill the above chat messages into a single summary message. Include names of speakers in priority\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    print(f\"chat_history: {chat_history}\")\n",
    "# summarization_chain = chat_history를 context로 받는 prompt + llama-3-ko\n",
    "    summarization_chain = summarization_prompt | chat #->chat_history를 요약하는 chain\n",
    "    summary_message = summarization_chain.invoke({\"chat_history\": stored_messages}) #summarized_message 생성\n",
    "    chat_history.clear() #요약 생성 후에는 메시지 초기화\n",
    "    chat_history.add_message(summary_message) #요약된 메시지를 chat_history에 더해주기\n",
    "    print(f\"summary_message: {summary_message}\")\n",
    "    return True\n",
    "\n",
    "chain_with_summarization = (\n",
    "    RunnablePassthrough.assign(messages_summarized = summarize_messages)\n",
    "    | chain_with_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  messages_summarized: RunnableLambda(summarize_messages)\n",
       "})\n",
       "| RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "    chat_history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "  }), config={'run_name': 'insert_history'})\n",
       "  | RunnableBinding(bound=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant. Answer all questions to the best of your ability.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "    | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x120bd6010>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x122bf2a50>, model_name='cognitivecomputations/dolphin-2.9-llama3-8b-gguf', temperature=0.2, openai_api_key=SecretStr('**********'), openai_api_base='http://localhost:1234/v1', openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x111f88a40>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function <lambda> at 0x111f88ae0>, input_messages_key='input', history_messages_key='chat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_history: Human: hey there! I'm Nemo.\n",
      "AI: Hello.\n",
      "Human: How are you today?\n",
      "AI: Fine thanks!\n",
      "summary_message: content='Nemo introduced himself and asked how I was doing, to which I replied positively.' response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 17, 'total_tokens': 34}, 'model_name': 'cognitivecomputations/dolphin-2.9-llama3-8b-gguf', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-4d59cea2-7898-4906-a5fc-43076c25fa7c-0'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The name of the speaker is Nemo.\\n', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20}, 'model_name': 'cognitivecomputations/dolphin-2.9-llama3-8b-gguf', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0c083f81-3481-4849-8f32-308d5b7e3edd-0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_summarization.invoke(\n",
    "    {\"input\": \"What is the name of speaker\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 역시 Open AI의 모델은 성능이 좋습니다\n",
    "## 대화내용\n",
    "- prompt:\"Distill the above chat messages into a single summary message. Include as many specific details as you can\" \n",
    "                \n",
    "- chat_history: Human: hey there! I'm Nemo.\n",
    "- AI: Hello.\n",
    "- Human: How are you today?\n",
    "- AI: Fine thanks!\n",
    "## chain_with_history까지는 gpt-3.5-turbo, llama-3-ko 모두 기존에 이야기 했던 이름 기억 가능\n",
    "## chain_with_summary에서는 위 대화에 대한 요약을 바탕으로 답변을 요구할 시, llama-3-ko에서 오류 발생\n",
    "- prompt와 input을 조정한 후 올바른 답변 가능\n",
    "    - 조정된 promt: \"Distill the above chat messages into a single summary message. Include names of speakers in priority\"\n",
    "    - 조정된 input query: \"What is the names of speaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Teddy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
