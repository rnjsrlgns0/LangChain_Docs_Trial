{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGì™€ Memory Chainì„ ëª¨ë‘ ì ìš©í•œ ì±—ë´‡ ë§Œë“¤ê¸°ë¥¼ ìœ„í•œ ì—°ìŠµì…ë‹ˆë‹¤ğŸ™‚\n",
    "# Chabots\n",
    " - llmì— ëŒ€í•œ ê°€ì¥ ì¸ê¸°ìˆëŠ” ì‚¬ìš©ì¼€ì´ìŠ¤\n",
    " - ì˜¤ëœê¸°ê°„ì˜ í•™ìŠµì„ ê±°ì¹˜ë©°, ê´€ë ¨ ì •ë³´ë¥¼ ì´ìš©, í˜„ì¬ìƒí™©ì— ëŒ€í•œ ëŒ€í™”ë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atchitecture\n",
    " - Chatbotì„ ë””ìì¸í•˜ê¸° ìœ„í•´ì„œëŠ” ì—¬ëŸ¬ ìš”ì†Œë¥¼ ê³ ë ¤í•´ì•¼í•¨\n",
    " - ë‹¹ì—°íˆ DB, Memory ë“± ëª¨ë“  ì •ë³´ë¥¼ ë„£ê³  promptë¥¼ ì œê³µí•˜ë©´ ì„±ëŠ¥ì€ ë†’ì•„ì§€ì§€ë§Œ ì†ë„, ìì› ë“± trade-offë¥¼ ìƒê°í•´ì•¼í•¨\n",
    " - ì•„ë˜ëŠ” ì¼ë°˜ì ì¸ chatbotì˜ ì•„í‚¤í…ì²˜\n",
    " ![Chabot Architecture](./imgs/Chatbot_Architecture.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbotì˜ ê¸°ë³¸ êµ¬ì„±ìš”ì†Œ\n",
    " - Chatmodels : ì‚¬ìš©ì ì¿¼ë¦¬ ë° ì œê³µ ë°ì´í„°ë¥¼ ê°€ì§€ê³  í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  LLMëª¨ë¸\n",
    " - PromptTemplates : ëª¨ë¸ì´ ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì„ ì˜ ë°›ì•„ë“¤ì´ê²Œ í•˜ê³ , ì¶œë ¥ë¬¼ì˜ ê²°ê³¼ë¥¼ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” í˜•íƒœë¡œ ë§Œë“¤ê¸° ìœ„í•œ template\n",
    " - ChatHistory : ì‚¬ìš©ìì™€ì˜ ëŒ€í™” ê¸°ë¡ì„ ì €ì¥, ë‹¤ìŒ ë‹µë³€ ì‹œ ì´ì „ì˜ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ìˆìŒ\n",
    " - Retrievers : ì‚¬ìš©ìì˜ ì¿¼ë¦¬ë¥¼ í†µí•´ ì ì •í•œ ë°ì´í„°ë¥¼ ì°¾ì•„ì„œ Chatmodelsì—ê²Œ ì œê³µí•´ì£¼ëŠ” ì—­í• \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../dot.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìš°ì„ ì€ GPT ëª¨ë¸ì„ ì‚¬ìš©í•´ë´…ë‹ˆë‹¤\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "                   base_url=\"http://localhost:1234/v1\",\n",
    "                   api_key=\"lm-studio\",\n",
    "                   model=\"teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf\",\n",
    "                   temperature = 0.0,\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Je adore le programme.', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 6, 'total_tokens': 12}, 'model_name': 'teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e3f24db3-ee62-470d-919c-f3002a2ca7cd-0')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import AIMessage as AIM\n",
    "\n",
    "chat.invoke(\n",
    "    [HumanMessage(content = \"Translate this sentence from English to French: I love programming.\")])\n",
    "\n",
    "#ì§ˆì˜ í˜•ì‹ ë³´ê¸°\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ìœ„ ìƒíƒœì˜ ì±—ë´‡ì€ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ì§€ ëª»í•©ë‹ˆë‹¤.\n",
    " - ê°€ì¥ ê°„ë‹¨í•œ ë°©ì‹ìœ¼ë¡œ ì´ì „ì˜ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê²Œë” chatbotì„ êµ¬ì„±í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human Messageì™€ AIMessageê°€ ì„ì—¬ìˆì„ ë•Œ Chatbotì€ ê°€ì¥ ìµœê·¼ì˜ Human Messageì— ëŒ€í•´ ëŒ€í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Je viens de dire que j\\'aime le codage, ce qui signifie en franÃ§ais \"I love programming\".'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AIMessage = chat.invoke(\n",
    "    [\n",
    "        HumanMessage(content = \"Translate this sentence from English to French: I love programming.\"),\n",
    "        AIM(content = \"J'aime le codage\"),\n",
    "        HumanMessage(content = \"What did you just say?\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "AIMessage.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PromptTemplateì„ ì£¼ë©´ Chatbotì´ ë”ìš± íš¨ìœ¨ì ìœ¼ë¡œ ê³¼ì œë¥¼ ì•Œì•„ë“¤ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- ìš°ë¦¬ëŠ” modelì— templateì„ ë„£ëŠ” ë°©ì‹ìœ¼ë¡œ ì´ë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# promptëŠ” ëª¨ë¸ì— ì˜ ì…ë ¥ë  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ì •ì˜í•´ì•¼í•©ë‹ˆë‹¤.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\"\n",
    "        ),\n",
    "        # MessagesPlaceholderëŠ” ëŒ€í™”ì˜ ì´ì „ ë©”ì‹œì§€ë“¤ì„ í¬í•¨í•˜ëŠ” ë³€ìˆ˜ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì±—ë´‡ì€ ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ì´í•´í•˜ê³ , ì—°ì†ì ì¸ ëŒ€í™”ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 'messages'ë¼ëŠ” ë³€ìˆ˜ ì´ë¦„ìœ¼ë¡œ ì´ì „ ë©”ì‹œì§€ë“¤ì„ ì°¸ì¡°í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "        MessagesPlaceholder(variable_name = \"messages\")\n",
    "])\n",
    "chain = prompt | chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Messageë“¤ ìì²´ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ì£¼ì–´ ê¸°ì–µí•˜ê²Œ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "- MessagePlaceholderëŠ” ëŒ€í™”ì˜ ì´ì „ ë©”ì‹œì§€ë“¤ì„ í¬í•¨í•˜ëŠ” ë³€ìˆ˜ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì±—ë´‡ì€ ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ì´í•´í•˜ê³ , ì—°ì†ì ì¸ ëŒ€í™”ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 'messages'ë¼ëŠ” ë³€ìˆ˜ ì´ë¦„ìœ¼ë¡œ ì´ì „ ë©”ì‹œì§€ë“¤ì„ ì°¸ì¡°í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Je viens de dire que j\\'adorer la programmation, ce qui signifie en franÃ§ais \"I love programming\".'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Translate this sentence from English to French: I love programming.\"),\n",
    "            AIM(content=\"J'adore la programmation.\"),\n",
    "            HumanMessage(content=\"What did you just say?\"),\n",
    "        ],\n",
    "    }\n",
    ").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message history\n",
    "- ì±„íŒ… ê¸°ë¡ì„ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ê°„í¸í•œ ë°©ë²•ìœ¼ë¡œ, ì±„íŒ… ë©”ì‹œì§€ë¥¼ ì €ì¥í•˜ê³  ë¶ˆëŸ¬ì˜¤ëŠ” ì—­í• ì„ í•˜ëŠ” MessageHistory í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "- ë‹¤ì–‘í•œ ë°ì´í„°ë² ì´ìŠ¤ì— ë©”ì‹œì§€ë¥¼ ì§€ì†ì ìœ¼ë¡œ ì €ì¥í•˜ëŠ” ë§ì€ ë‚´ì¥ ë©”ì‹œì§€ ê¸°ë¡ í†µí•© ê¸°ëŠ¥ì´ ìˆì§€ë§Œ, ì´ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œì—ì„œëŠ” ë©”ëª¨ë¦¬ ë‚´ì—ì„œ ì‘ë™í•˜ëŠ” ë°ëª¨ ë©”ì‹œì§€ ê¸°ë¡ì¸ ChatMessageHistoryë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi'), HumanMessage(content=\"what's up?\")]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ChatMessagehistoryë¥¼ ì´ìš©í•˜ë©´ ê°„ë‹¨í•œ ë°©ì‹ìœ¼ë¡œ Memoryë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ê°€ë ¹,\n",
    "from langchain.memory import ChatMessageHistory\n",
    "#ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "chathistory = ChatMessageHistory()\n",
    "#add_user_message()ë©”ì„œë“œë¥¼ ì´ìš©í•œ ìœ ì €ë©”ì‹œì§€ ì‚½ì…\n",
    "chathistory.add_user_message('hi')\n",
    "#add_ai_message()ë©”ì„œë“œë¥¼ ì´ìš©í•œ ìœ ì €ë©”ì‹œì§€ ì‚½ì…\n",
    "chathistory.add_user_message(\"what's up?\")\n",
    "\n",
    "chathistory.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ìƒì„±í•œ chatmessagehistoryë¥¼ chainì˜ Messageplaceholderì— ë„£ì–´ ëŒ€í™”ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Tu es un assistant utile\\n- Tu rÃ©ponds aux questions au mieux de tes capacitÃ©s\\n- Tu as besoin d\\'aide pour traduire une phrase en franÃ§ais\\n- La phrase est \"J\\'aimer le programme\"\\n- Tu veux rÃ©sumer la conversation sous forme de points saillants'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chathistory.add_user_message(\n",
    "    \"Summarize the conversation in bullet points.\"\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"messages\": chathistory.messages})\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ìœ„ì—ì„œ ë§Œë“  ì±—ë´‡ì„ ì´ìš©í•´ RAGë¥¼ ì ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì„œëŠ” webbaseloaderë¥¼ ì´ìš©í•´ ì¸í„°ë„·ì— ìˆëŠ” ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¤ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\n\\n\\nGet started with LangSmith | \\uf8ffÃ¼Â¶ÃºÃ”âˆÃ¨\\uf8ffÃ¼Ãµâ€ Ã”âˆÃ¨ LangSmith\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€šÃ„Ã£PythonTypeScriptpip install -U langsmithyarn add langchain langsmith2. Create an API keyâ€šÃ„Ã£To create an API key head to the Settings page. Then click Create API Key.3. Set up your environmentâ€šÃ„Ã£Shellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it\\'s not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>4. Log your first traceâ€šÃ„Ã£We provide multiple ways to log traces to LangSmith. Below, we\\'ll highlight\\nhow to use traceable. See more on the Annotate code for tracing page.PythonTypeScriptimport openaifrom langsmith.wrappers import wrap_openaifrom langsmith import traceable# Auto-trace LLM calls in-contextclient = wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str):    result = client.chat.completions.create(        messages=[{\"role\": \"user\", \"content\": user_input}],        model=\"gpt-3.5-turbo\"    )    return result.choices[0].message.contentpipeline(\"Hello, world!\")# Out:  Hello there! How can I assist you today?import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: \"user\", content: user_input }],        model: \"gpt-3.5-turbo\",    });    return result.choices[0].message.content;});await pipeline(\"Hello, world!\")// Out: Hello there! How can I assist you today?View a sample output trace.Learn more about tracing in the how-to guides.5. Run your first evaluationâ€šÃ„Ã£Evaluation requires a system to test, data to serve as test cases, and optionally evaluators to grade the results. Here we use a built-in accuracy evaluator.PythonTypeScriptfrom langsmith import Clientfrom langsmith.evaluation import evaluateclient = Client()# Define dataset: these are your test casesdataset_name = \"Sample Dataset\"dataset = client.create_dataset(dataset_name, description=\"A sample dataset in LangSmith.\")client.create_examples(    inputs=[        {\"postfix\": \"to LangSmith\"},        {\"postfix\": \"to Evaluations in LangSmith\"},    ],    outputs=[        {\"output\": \"Welcome to LangSmith\"},        {\"output\": \"Welcome to Evaluations in LangSmith\"},    ],    dataset_id=dataset.id,)# Define your evaluatordef exact_match(run, example):    return {\"score\": run.outputs[\"output\"] == example.outputs[\"output\"]}experiment_results = evaluate(    lambda input: \"Welcome \" + input[\\'postfix\\'], # Your AI system goes here    data=dataset_name, # The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix=\"sample-experiment\", # The name of the experiment    metadata={      \"version\": \"1.0.0\",      \"revision_id\": \"beta\"    },)import { Client, Run, Example } from \"langsmith\";import { evaluate } from \"langsmith/evaluation\";import { EvaluationResult } from \"langsmith/evaluation\";const client = new Client();// Define dataset: these are your test casesconst datasetName = \"Sample Dataset\";const dataset = await client.createDataset(datasetName, {  description: \"A sample dataset in LangSmith.\",});await client.createExamples({  inputs: [    { postfix: \"to LangSmith\" },    { postfix: \"to Evaluations in LangSmith\" },  ],  outputs: [    { output: \"Welcome to LangSmith\" },    { output: \"Welcome to Evaluations in LangSmith\" },  ],  datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async (  run: Run,  example: Example): Promise<EvaluationResult> => {  return {    key: \"exact_match\",    score: run.outputs?.output === example?.outputs?.output,  };};await evaluate(  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),  {    data: datasetName,    evaluators: [exactMatch],    metadata: {      version: \"1.0.0\",      revision_id: \"beta\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â¬Â© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffÃ¼Â¶ÃºÃ”âˆÃ¨\\uf8ffÃ¼Ãµâ€ Ã”âˆÃ¨ LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en'})]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ê°€ì ¸ì˜¨ ë°ì´í„°ëŠ” ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ë§¤ìš° ë§ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitter\n",
    "ë§ì€ ì–‘ì˜ ë¬¸ì„œë¥¼ í•œë²ˆì— contextë¡œ ì œê³µí•˜ëŠ” ê²ƒì€ ë§ì€ ìì›ì„ ìš”êµ¬í•˜ê±°ë‚˜ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Teddy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
