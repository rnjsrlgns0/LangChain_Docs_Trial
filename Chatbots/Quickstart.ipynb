{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG와 Memory Chain을 모두 적용한 챗봇 만들기를 위한 연습입니다🙂\n",
    "# Chabots\n",
    " - llm에 대한 가장 인기있는 사용케이스\n",
    " - 오랜기간의 학습을 거치며, 관련 정보를 이용, 현재상황에 대한 대화를 나눌 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atchitecture\n",
    " - Chatbot을 디자인하기 위해서는 여러 요소를 고려해야함\n",
    " - 당연히 DB, Memory 등 모든 정보를 넣고 prompt를 제공하면 성능은 높아지지만 속도, 자원 등 trade-off를 생각해야함\n",
    " - 아래는 일반적인 chatbot의 아키텍처\n",
    " ![Chabot Architecture](./imgs/Chatbot_Architecture.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot의 기본 구성요소\n",
    " - Chatmodels : 사용자 쿼리 및 제공 데이터를 가지고 텍스트를 생성할 LLM모델\n",
    " - PromptTemplates : 모델이 사용자 요구사항을 잘 받아들이게 하고, 출력물의 결과를 사용자가 원하는 형태로 만들기 위한 template\n",
    " - ChatHistory : 사용자와의 대화 기록을 저장, 다음 답변 시 이전의 대화를 바탕으로 답변을 생성할 수 있음\n",
    " - Retrievers : 사용자의 쿼리를 통해 적정한 데이터를 찾아서 Chatmodels에게 제공해주는 역할\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../dot.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우선은 GPT 모델을 사용해봅니다\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "                   base_url=\"http://localhost:1234/v1\",\n",
    "                   api_key=\"lm-studio\",\n",
    "                   model=\"teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf\",\n",
    "                   temperature = 0.0,\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Je adore le programme.', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 6, 'total_tokens': 12}, 'model_name': 'teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e3f24db3-ee62-470d-919c-f3002a2ca7cd-0')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import AIMessage as AIM\n",
    "\n",
    "chat.invoke(\n",
    "    [HumanMessage(content = \"Translate this sentence from English to French: I love programming.\")])\n",
    "\n",
    "#질의 형식 보기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 위 상태의 챗봇은 이전 대화를 기억하지 못합니다.\n",
    " - 가장 간단한 방식으로 이전의 대화를 기억하게끔 chatbot을 구성해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human Message와 AIMessage가 섞여있을 때 Chatbot은 가장 최근의 Human Message에 대해 대화를 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Je viens de dire que j\\'aime le codage, ce qui signifie en français \"I love programming\".'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AIMessage = chat.invoke(\n",
    "    [\n",
    "        HumanMessage(content = \"Translate this sentence from English to French: I love programming.\"),\n",
    "        AIM(content = \"J'aime le codage\"),\n",
    "        HumanMessage(content = \"What did you just say?\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "AIMessage.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PromptTemplate을 주면 Chatbot이 더욱 효율적으로 과제를 알아들을 수 있습니다.\n",
    "- 우리는 model에 template을 넣는 방식으로 이를 해결할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# prompt는 모델에 잘 입력될 수 있는 형태로 정의해야합니다.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\"\n",
    "        ),\n",
    "        # MessagesPlaceholder는 대화의 이전 메시지들을 포함하는 변수입니다. 이를 통해 챗봇은 이전 대화의 맥락을 이해하고, 연속적인 대화를 진행할 수 있습니다. 'messages'라는 변수 이름으로 이전 메시지들을 참조하게 됩니다.\n",
    "        MessagesPlaceholder(variable_name = \"messages\")\n",
    "])\n",
    "chain = prompt | chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message들 자체를 메타데이터로 주어 기억하게 할 수도 있습니다.\n",
    "- MessagePlaceholder는 대화의 이전 메시지들을 포함하는 변수입니다. 이를 통해 챗봇은 이전 대화의 맥락을 이해하고, 연속적인 대화를 진행할 수 있습니다. 'messages'라는 변수 이름으로 이전 메시지들을 참조하게 됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Je viens de dire que j\\'adorer la programmation, ce qui signifie en français \"I love programming\".'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Translate this sentence from English to French: I love programming.\"),\n",
    "            AIM(content=\"J'adore la programmation.\"),\n",
    "            HumanMessage(content=\"What did you just say?\"),\n",
    "        ],\n",
    "    }\n",
    ").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message history\n",
    "- 채팅 기록을 관리하기 위한 간편한 방법으로, 채팅 메시지를 저장하고 불러오는 역할을 하는 MessageHistory 클래스를 사용할 수 있습니다. \n",
    "- 다양한 데이터베이스에 메시지를 지속적으로 저장하는 많은 내장 메시지 기록 통합 기능이 있지만, 이 빠른 시작 가이드에서는 메모리 내에서 작동하는 데모 메시지 기록인 ChatMessageHistory를 사용하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi'), HumanMessage(content=\"what's up?\")]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ChatMessagehistory를 이용하면 간단한 방식으로 Memory를 관리할 수 있습니다 가령,\n",
    "from langchain.memory import ChatMessageHistory\n",
    "#인스턴스 생성\n",
    "chathistory = ChatMessageHistory()\n",
    "#add_user_message()메서드를 이용한 유저메시지 삽입\n",
    "chathistory.add_user_message('hi')\n",
    "#add_ai_message()메서드를 이용한 유저메시지 삽입\n",
    "chathistory.add_user_message(\"what's up?\")\n",
    "\n",
    "chathistory.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 생성한 chatmessagehistory를 chain의 Messageplaceholder에 넣어 대화를 진행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Tu es un assistant utile\\n- Tu réponds aux questions au mieux de tes capacités\\n- Tu as besoin d\\'aide pour traduire une phrase en français\\n- La phrase est \"J\\'aimer le programme\"\\n- Tu veux résumer la conversation sous forme de points saillants'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chathistory.add_user_message(\n",
    "    \"Summarize the conversation in bullet points.\"\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"messages\": chathistory.messages})\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 위에서 만든 챗봇을 이용해 RAG를 적용해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서는 webbaseloader를 이용해 인터넷에 있는 문서를 가져오도록 하겠습니다.\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\n\\n\\nGet started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmith‚ÄãPythonTypeScriptpip install -U langsmithyarn add langchain langsmith2. Create an API key‚ÄãTo create an API key head to the Settings page. Then click Create API Key.3. Set up your environment‚ÄãShellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it\\'s not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>4. Log your first trace‚ÄãWe provide multiple ways to log traces to LangSmith. Below, we\\'ll highlight\\nhow to use traceable. See more on the Annotate code for tracing page.PythonTypeScriptimport openaifrom langsmith.wrappers import wrap_openaifrom langsmith import traceable# Auto-trace LLM calls in-contextclient = wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str):    result = client.chat.completions.create(        messages=[{\"role\": \"user\", \"content\": user_input}],        model=\"gpt-3.5-turbo\"    )    return result.choices[0].message.contentpipeline(\"Hello, world!\")# Out:  Hello there! How can I assist you today?import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: \"user\", content: user_input }],        model: \"gpt-3.5-turbo\",    });    return result.choices[0].message.content;});await pipeline(\"Hello, world!\")// Out: Hello there! How can I assist you today?View a sample output trace.Learn more about tracing in the how-to guides.5. Run your first evaluation‚ÄãEvaluation requires a system to test, data to serve as test cases, and optionally evaluators to grade the results. Here we use a built-in accuracy evaluator.PythonTypeScriptfrom langsmith import Clientfrom langsmith.evaluation import evaluateclient = Client()# Define dataset: these are your test casesdataset_name = \"Sample Dataset\"dataset = client.create_dataset(dataset_name, description=\"A sample dataset in LangSmith.\")client.create_examples(    inputs=[        {\"postfix\": \"to LangSmith\"},        {\"postfix\": \"to Evaluations in LangSmith\"},    ],    outputs=[        {\"output\": \"Welcome to LangSmith\"},        {\"output\": \"Welcome to Evaluations in LangSmith\"},    ],    dataset_id=dataset.id,)# Define your evaluatordef exact_match(run, example):    return {\"score\": run.outputs[\"output\"] == example.outputs[\"output\"]}experiment_results = evaluate(    lambda input: \"Welcome \" + input[\\'postfix\\'], # Your AI system goes here    data=dataset_name, # The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix=\"sample-experiment\", # The name of the experiment    metadata={      \"version\": \"1.0.0\",      \"revision_id\": \"beta\"    },)import { Client, Run, Example } from \"langsmith\";import { evaluate } from \"langsmith/evaluation\";import { EvaluationResult } from \"langsmith/evaluation\";const client = new Client();// Define dataset: these are your test casesconst datasetName = \"Sample Dataset\";const dataset = await client.createDataset(datasetName, {  description: \"A sample dataset in LangSmith.\",});await client.createExamples({  inputs: [    { postfix: \"to LangSmith\" },    { postfix: \"to Evaluations in LangSmith\" },  ],  outputs: [    { output: \"Welcome to LangSmith\" },    { output: \"Welcome to Evaluations in LangSmith\" },  ],  datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async (  run: Run,  example: Example): Promise<EvaluationResult> => {  return {    key: \"exact_match\",    score: run.outputs?.output === example?.outputs?.output,  };};await evaluate(  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),  {    data: datasetName,    evaluators: [exactMatch],    metadata: {      version: \"1.0.0\",      revision_id: \"beta\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en'})]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가져온 데이터는 대부분의 경우 매우 많은 양의 데이터를 가지고 있습니다.\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitter\n",
    "많은 양의 문서를 한번에 context로 제공하는 것은 많은 자원을 요구하거나 불가능한 경우가 많습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Teddy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
